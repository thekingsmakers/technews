{
  "name": "TechPulse Ultimate Scraper v3 - Full Website Scraping",
  "nodes": [
    {
      "parameters": {
        "values": {
          "boolean": [
            {
              "name": "useAdvancedScraping",
              "value": true
            }
          ],
          "string": [
            {
              "name": "targetUrls",
              "value": "https://www.computerweekly.com\nhttps://arstechnica.com"
            },
            {
              "name": "maxArticlesPerSite",
              "value": "10"
            },
            {
              "name": "scrapeDelay",
              "value": "3000"
            },
            {
              "name": "userAgent",
              "value": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            },
            {
              "name": "proxyList",
              "value": ""
            }
          ]
        },
        "options": {}
      },
      "id": "workflow-config-v3",
      "name": "Advanced Scraper Configuration",
      "type": "n8n-nodes-base.set",
      "typeVersion": 1,
      "position": [
        240,
        200
      ]
    },
    {
      "parameters": {
        "functionCode": "const config = $node[\"Advanced Scraper Configuration\"].json;\nconst urlList = config.targetUrls.split('\\n').filter(url => url.trim());\nconst maxArticles = parseInt(config.maxArticlesPerSite || '10');\nconst scrapeDelay = parseInt(config.scrapeDelay || '3000');\nconst userAgent = config.userAgent || 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36';\n\nconst proxies = config.proxyList ? config.proxyList.split('\\n').filter(p => p.trim()) : [];\n\n// Create processing objects for each URL\nreturn urlList.map(url => ({\n  json: {\n    targetUrl: url.trim(),\n    maxArticles,\n    scrapeDelay,\n    userAgent,\n    proxies,\n    processingId: Math.random().toString(36).substr(2, 9),\n    timestamp: new Date().toISOString()\n  }\n}));\n\nconsole.log(`Prepared ${urlList.length} websites for advanced scraping`);"
      },
      "id": "url-processor-v3",
      "name": "Process Target URLs",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        440,
        200
      ]
    },
    {
      "parameters": {
        "functionCode": "const input = $input.all()[0];\nconst config = input.json;\nconst targetUrl = config.targetUrl;\n\n// Advanced HTML structure analysis\nconst analyzeStructure = (htmlContent, url) => {\n  const domain = url.replace('https://', '').replace('http://', '').split('/')[0].replace('www.', '');\n  \n  // Enhanced site patterns with full content selectors\n  const enhancedPatterns = {\n    'computerweekly.com': {\n      articleSelector: 'article, .article, .post, .news-item',\n      titleSelector: 'h1, .article-title, .post-title',\n      urlSelector: 'h1 a, .article-title a, .post-title a',\n      summarySelector: '.article-intro, .post-excerpt, p:first-of-type',\n      dateSelector: 'time, .article-date, .post-date, [datetime]',\n      categorySelector: '.article-category, .post-category',\n      contentSelector: 'article .content, .article-body, .post-content',\n      authorSelector: '.article-author, .post-author, .byline',\n      imageSelector: 'article img, .article-image img',\n      confidence: 1.0,\n      fullContentStrategy: 'article-body'\n    },\n    'arstechnica.com': {\n      articleSelector: 'article.tease, article, .tease',\n      titleSelector: 'h1, h2 a, .article-title a',\n      urlSelector: 'h1 a, h2 a, .article-title a',\n      summarySelector: '.article-intro, .excerpt, p:first-child',\n      dateSelector: 'time, .article-date, [datetime]',\n      categorySelector: '.article-category, .tag',\n      contentSelector: '.article-content, .post-content, article p',\n      authorSelector: '.article-author, .byline',\n      imageSelector: 'article img, .article-image img',\n      confidence: 1.0,\n      fullContentStrategy: 'article-content'\n    },\n    'techcrunch.com': {\n      articleSelector: 'article.post-block, .post-block',\n      titleSelector: 'h2.post-block__title a, .post-block__title a',\n      urlSelector: 'h2.post-block__title a, .post-block__title a',\n      summarySelector: '.post-block__content',\n      dateSelector: '.river-byline__time',\n      categorySelector: '.post-block__category',\n      contentSelector: 'article .content, .article-content',\n      authorSelector: '.article-author, .byline',\n      imageSelector: 'article img, .featured-image img',\n      confidence: 1.0,\n      fullContentStrategy: 'article-body'\n    }\n  };\n  \n  if (enhancedPatterns[domain]) {\n    return enhancedPatterns[domain];\n  }\n  \n  // AI-powered adaptive analysis for unknown sites\n  const parser = new DOMParser();\n  const doc = parser.parseFromString(htmlContent, 'text/html');\n  \n  // Test multiple selector combinations\n  const selectorCombos = [\n    { article: 'article', title: 'h1, h2', confidence: 0.9 },\n    { article: '.post, .article', title: '.post-title, .article-title h1, h2', confidence: 0.8 },\n    { article: '.card, .entry', title: '.card-title, .entry-title h1, h2', confidence: 0.7 },\n    { article: '.news-item, .story', title: '.news-title, .story-title h1, h2', confidence: 0.6 }\n  ];\n  \n  for (const combo of selectorCombos) {\n    const articles = doc.querySelectorAll(combo.article);\n    if (articles.length > 0 && articles.length <= config.maxArticles * 3) {\n      return {\n        articleSelector: combo.article,\n        titleSelector: combo.title,\n        urlSelector: 'a[href]',\n        summarySelector: 'p, .summary, .excerpt',\n        dateSelector: 'time, .date, .published, [datetime]',\n        categorySelector: '.category, .tag, .topic',\n        contentSelector: 'article p, .content p, .post-content p',\n        authorSelector: '.author, .byline',\n        imageSelector: 'img',\n        confidence: combo.confidence,\n        fullContentStrategy: 'generic'\n      };\n    }\n  }\n  \n  // Ultimate fallback\n  return {\n    articleSelector: 'article, .article, .post, .entry',\n    titleSelector: 'h1, h2, h3, .title',\n    urlSelector: 'a[href]',\n    summarySelector: 'p, .summary',\n    dateSelector: 'time, .date',\n    categorySelector: '.category, .tag',\n    contentSelector: 'article p, .content p',\n    authorSelector: '.author',\n    imageSelector: 'img',\n    confidence: 0.3,\n    fullContentStrategy: 'fallback'\n  };\n};\n\n// Detect if site needs JavaScript rendering\nconst needsJavaScript = (url) => {\n  const spaSites = ['bloomberg.com', 'wsj.com', 'ft.com', 'nytimes.com', 'washingtonpost.com'];\n  const dynamicSites = ['medium.com', 'substack.com', 'ghost.org'];\n  return spaSites.some(site => url.includes(site)) || dynamicSites.some(site => url.includes(site));\n};\n\nreturn {\n  json: {\n    ...config,\n    siteAnalysis: analyzeStructure,\n    needsJavaScript: needsJavaScript(targetUrl),\n    analysisComplete: true\n  }\n};"
      },
      "id": "ai-structure-analyzer-v3",
      "name": "AI Structure Analysis",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        640,
        200
      ]
    },
    {
      "parameters": {
        "functionCode": "=// Enhanced scraper selection with anti-detection\nconst input = $input.all()[0];\nconst config = input.json;\n\n// Advanced scraper selection logic\nlet scraperType = 'enhanced-python';\nlet pythonCode = '';\nlet jsCode = '';\n\nif (config.needsJavaScript) {\n  scraperType = 'stealth-javascript';\n  \n  // Generate advanced JavaScript scraper with anti-detection\n  jsCode = `\nconst puppeteer = require('puppeteer-extra');\nconst StealthPlugin = require('puppeteer-extra-plugin-stealth');\n\npuppeteer.use(StealthPlugin());\n\nasync function scrapeWithAdvancedJS(url, maxArticles, proxyList = []) {\n  let browser;\n  try {\n    const args = [\n      '--no-sandbox',\n      '--disable-setuid-sandbox',\n      '--disable-dev-shm-usage',\n      '--disable-accelerated-2d-canvas',\n      '--no-first-run',\n      '--no-zygote',\n      '--disable-gpu',\n      '--disable-web-security',\n      '--disable-features=VizDisplayCompositor'\n    ];\n    \n    // Add proxy if available\n    if (proxyList && proxyList.length > 0) {\n      const proxy = proxyList[Math.floor(Math.random() * proxyList.length)];\n      args.push(\`--proxy-server=\${proxy}\`);\n    }\n    \n    browser = await puppeteer.launch({\n      headless: true,\n      args,\n      ignoreDefaultArgs: ['--disable-extensions']\n    });\n    \n    const page = await browser.newPage();\n    \n    // Rotate user agents\n    const userAgents = [\n      'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n      'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n      'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n    ];\n    \n    await page.setUserAgent(userAgents[Math.floor(Math.random() * userAgents.length)]);\n    \n    // Set viewport to desktop size\n    await page.setViewport({ width: 1920, height: 1080 });\n    \n    // Add anti-detection measures\n    await page.evaluateOnNewDocument(() => {\n      Object.defineProperty(navigator, 'webdriver', { get: () => undefined });\n      Object.defineProperty(navigator, 'plugins', {\n        get: () => [\n          { name: 'Chrome PDF Plugin', description: 'Portable Document Format', filename: 'internal-pdf-viewer' },\n          { name: 'Chrome PDF Viewer', description: '', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai' }\n        ]\n      });\n      \n      Object.defineProperty(navigator, 'languages', {\n        get: () => ['en-US', 'en']\n      });\n    });\n    \n    console.log(\`Navigating to \${url} with stealth mode...\`);\n    await page.goto(url, {\n      waitUntil: 'networkidle2',\n      timeout: 60000\n    });\n    \n    // Wait for dynamic content\n    await page.waitForTimeout(5000);\n    \n    // Scroll to load more content\n    await page.evaluate(() => {\n      window.scrollTo(0, document.body.scrollHeight);\n    });\n    await page.waitForTimeout(2000);\n    \n    // Extract articles with enhanced selectors\n    const articles = await page.evaluate((max) => {\n      const advancedSelectors = [\n        'article:not(.tease)',\n        '.post:not(.post-block)',\n        '.article:not(.tease)',\n        '.entry:not(.tease)',\n        '.story:not(.tease)'\n      ];\n      \n      let foundArticles = [];\n      \n      for (const selector of advancedSelectors) {\n        const elements = document.querySelectorAll(selector);\n        if (elements.length > 0 && elements.length <= max * 2) {\n          foundArticles = Array.from(elements).slice(0, max).map(el => ({\n            title: el.querySelector('h1, h2, .title')?.textContent?.trim() || '',\n            url: el.querySelector('h1 a, h2 a, .title a')?.href || '',\n            summary: el.querySelector('p, .summary, .excerpt')?.textContent?.trim() || '',\n            date: el.querySelector('time, .date, [datetime]')?.getAttribute('datetime') || \n                  el.querySelector('time, .date, [datetime]')?.textContent?.trim() || '',\n            category: el.querySelector('.category, .tag, .topic')?.textContent?.trim() || '',\n            author: el.querySelector('.author, .byline')?.textContent?.trim() || '',\n            imageUrl: el.querySelector('img')?.src || ''\n          }));\n          \n          foundArticles = foundArticles.filter(a => a.title && a.url);\n          if (foundArticles.length > 0) {\n            console.log(\`Found \${foundArticles.length} articles using selector: \${selector}\`);\n            break;\n          }\n        }\n      }\n      \n      return foundArticles;\n    }, maxArticles);\n    \n    console.log(\`Successfully scraped \${articles.length} articles from \${url}\`);\n    return articles;\n    \n  } catch (error) {\n    console.error(\`Error scraping \${url}:\`, error);\n    return [];\n  } finally {\n    if (browser) {\n      await browser.close();\n    }\n  }\n}\n\n// Execute advanced scraping\nscrapeWithAdvancedJS('${config.targetUrl}', ${config.maxArticles}, ${JSON.stringify(config.proxies)});\n`;\n} else {\n  scraperType = 'enhanced-python';\n  \n  // Generate enhanced Python scraper with anti-detection\n  pythonCode = `\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nimport random\nimport json\nimport urllib3\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\n\nTARGET_URL = \"${config.targetUrl}\"\nMAX_ARTICLES = ${config.maxArticles}\nSCRAPE_DELAY = ${config.scrapeDelay}\nUSER_AGENT = \"${config.userAgent}\"\n\nclass AdvancedNewsScraper:\n    def __init__(self):\n        self.session = requests.Session()\n        \n        # Enhanced session configuration\n        retry_strategy = Retry(\n            total=3,\n            status_forcelist=[429, 500, 502, 503, 504],\n            method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"],\n            backoff_factor=1\n        )\n        \n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        self.session.mount(\"http://\", adapter)\n        self.session.mount(\"https://\", adapter)\n        \n        # Anti-detection headers\n        self.user_agents = [\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'\n        ]\n        \n        self.session.headers.update({\n            'User-Agent': USER_AGENT,\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip, deflate',\n            'DNT': '1',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1',\n            'Sec-Fetch-Dest': 'document',\n            'Sec-Fetch-Mode': 'navigate',\n            'Sec-Fetch-Site': 'none',\n            'Cache-Control': 'max-age=0'\n        })\n    \n    def scrape_articles(self):\n        try:\n            # Rotate user agent for each request\n            self.session.headers['User-Agent'] = random.choice(self.user_agents)\n            \n            print(f\"Fetching {TARGET_URL} with anti-detection measures...\")\n            \n            response = self.session.get(TARGET_URL, timeout=30)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Remove common anti-scraping elements\n            for script in soup([\"script\", \"style\"]):\n                script.decompose()\n            \n            articles = []\n            \n            # Enhanced site-specific selectors\n            domain = TARGET_URL.replace('https://', '').replace('http://', '').split('/')[0].replace('www.', '')\n            \n            if 'computerweekly.com' in domain:\n                selectors = ['article', '.article', '.post', '.news-item']\n            elif 'arstechnica.com' in domain:\n                selectors = ['article.tease', 'article', '.tease']\n            elif 'techcrunch.com' in domain:\n                selectors = ['article.post-block', '.post-block']\n            else:\n                selectors = ['article', '.article', '.post', '.card', '.entry']\n            \n            for selector in selectors:\n                elements = soup.select(selector)\n                if elements and len(elements) <= MAX_ARTICLES * 3:\n                    for elem in elements[:MAX_ARTICLES]:\n                        title = elem.select_one('h1, h2, h3, .title, .headline')?.get_text().strip()\n                        url_elem = elem.select_one('a[href]')\n                        url = url_elem.get('href') if url_elem else ''\n                        \n                        if not url:\n                            continue\n                            \n                        # Make absolute URL\n                        if url.startswith('/'):\n                            base_url = TARGET_URL.rstrip('/')\n                            url = base_url + url\n                        elif not url.startswith('http'):\n                            base_url = TARGET_URL.rstrip('/')\n                            url = base_url + '/' + url\n                        \n                        summary = elem.select_one('p, .summary, .excerpt, .description')?.get_text().strip()\n                        \n                        date_elem = elem.select_one('time, .date, .published, [datetime]')\n                        published_at = None\n                        if date_elem:\n                            published_at = date_elem.get('datetime') or date_elem.get_text().strip()\n                        \n                        category = elem.select_one('.category, .tag, .topic')?.get_text().strip()\n                        author = elem.select_one('.author, .byline')?.get_text().strip()\n                        \n                        img_elem = elem.select_one('img')\n                        image_url = None\n                        if img_elem:\n                            image_url = img_elem.get('src') or img_elem.get('data-src')\n                            if image_url and not image_url.startswith('http'):\n                                base_url = TARGET_URL.rstrip('/')\n                                image_url = base_url + '/' + image_url.lstrip('/') if image_url.startswith('/') else base_url + '/' + image_url\n                        \n                        if title and url:\n                            articles.append({\n                                'title': title,\n                                'url': url,\n                                'summary': summary or '',\n                                'published_at': published_at,\n                                'category': category or 'Technology',\n                                'author': author or '',\n                                'image_url': image_url,\n                                'source': domain\n                            })\n                    \n                    if len(articles) >= MAX_ARTICLES:\n                        break\n                \n                if len(articles) > 0:\n                    break\n            \n            print(f\"Found {len(articles)} articles\")\n            return articles[:MAX_ARTICLES]\n            \n        except Exception as e:\n            print(f\"Error scraping {TARGET_URL}: {e}\")\n            return []\n    \n    def scrape_full_content(self, article_url):\n        try:\n            time.sleep(SCRAPE_DELAY / 1000)\n            \n            # Rotate user agent\n            self.session.headers['User-Agent'] = random.choice(self.user_agents)\n            \n            print(f\"Scraping full content from {article_url}\")\n            response = self.session.get(article_url, timeout=30)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Remove unwanted elements\n            for unwanted in soup.select('script, style, nav, header, footer, aside, .ad, .advertisement, .sidebar, .social-share, .related-articles, .comments'):\n                unwanted.decompose()\n            \n            # Enhanced content extraction\n            content_selectors = [\n                'article .content-body, article .post-content, article .entry-content',\n                '.article-content, .post-content, .entry-content',\n                'article p, .article-body p, .post-body p',\n                'main article p, main .content p',\n                '.content p, .entry p'\n            ]\n            \n            full_content = ''\n            for selector in content_selectors:\n                content_elements = soup.select(selector)\n                if content_elements:\n                    paragraphs = []\n                    for elem in content_elements:\n                        if elem.name == 'p':\n                            text = elem.get_text().strip()\n                            if len(text) > 50:\n                                paragraphs.append(text)\n                        elif elem.find_all('p'):\n                            for p in elem.find_all('p'):\n                                text = p.get_text().strip()\n                                if len(text) > 50:\n                                    paragraphs.append(text)\n                    \n                    if paragraphs:\n                        full_content = ' '.join(paragraphs)\n                        break\n            \n            # Fallback to article element\n            if not full_content:\n                article_elem = soup.select_one('article')\n                if article_elem:\n                    paragraphs = article_elem.find_all('p')\n                    full_content = ' '.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 50])\n            \n            # Clean and limit content\n            import re\n            full_content = re.sub(r'\\s+', ' ', full_content).strip()\n            \n            print(f\"Extracted {len(full_content)} characters of content\")\n            return full_content[:15000] if full_content else \"Content not available\"\n            \n        except Exception as e:\n            print(f\"Error scraping full content from {article_url}: {e}\")\n            return \"Content not available\"\n\n# Execute enhanced scraping\nscraper = AdvancedNewsScraper()\narticles = scraper.scrape_articles()\n\n# Scrape full content for each article\nfor article in articles:\n    article['content'] = scraper.scrape_full_content(article['url'])\n    article['slug'] = re.sub(r'[^a-zA-Z0-9]+', '-', article['title'].lower()).strip('-')\n    article['tags'] = ['news', article['category'].lower().replace(' ', '-'), article['source']]\n\nprint(json.dumps(articles))\n`;\n}\n\nreturn {\n  json: {\n    ...config,\n    scraperType,\n    pythonCode,\n    jsCode,\n    scraperSelected: true\n  }\n};"
      },
      "id": "enhanced-scraper-selector",
      "name": "Enhanced Scraper Selector",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        840,
        200
      ]
    },
    {
      "parameters": {
        "functionCode": "=Python Code",
        "mode": "runOnceForAllItems"
      },
      "id": "enhanced-python-executor",
      "name": "Enhanced Python Scraper",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1040,
        120
      ]
    },
    {
      "parameters": {
        "functionCode": "=JavaScript Code",
        "mode": "runOnceForAllItems"
      },
      "id": "enhanced-js-executor",
      "name": "Enhanced JavaScript Scraper",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1040,
        280
      ]
    },
    {
      "parameters": {
        "functionCode": "const inputData = $input.all()[0];\nlet articles = [];\n\ntry {\n  const config = inputData.json;\n  const scraperType = config.scraperType;\n  \n  console.log(`Processing data from ${scraperType} scraper for ${config.targetUrl}`);\n  \n  let scrapedData;\n  if (scraperType === 'enhanced-python') {\n    scrapedData = $node[\"Enhanced Python Scraper\"].json;\n  } else {\n    scrapedData = $node[\"Enhanced JavaScript Scraper\"].json;\n  }\n  \n  // Enhanced data validation and processing\n  if (Array.isArray(scrapedData)) {\n    articles = scrapedData;\n  } else if (typeof scrapedData === 'string') {\n    try {\n      articles = JSON.parse(scrapedData);\n    } catch (e) {\n      console.error('JSON parse error:', e);\n      articles = [];\n    }\n  } else if (scrapedData && typeof scrapedData === 'object') {\n    if (scrapedData.data && Array.isArray(scrapedData.data)) {\n      articles = scrapedData.data;\n    } else if (scrapedData.articles) {\n      articles = scrapedData.articles;\n    } else {\n      articles = [scrapedData];\n    }\n  }\n  \n  // Quality filtering with enhanced validation\n  const validArticles = articles.filter(article => {\n    const hasRequiredFields = article && \n      typeof article.title === 'string' && \n      article.title.trim().length >= 10 && \n      typeof article.url === 'string' && \n      article.url.trim().length > 0;\n      \n    const hasContent = article.content && \n      typeof article.content === 'string' && \n      article.content.trim().length > 100;\n      \n    const isNotAdvertisement = !article.title?.toLowerCase().includes('advertisement') && \n      !article.title?.toLowerCase().includes('sponsored');\n      \n    return hasRequiredFields && hasContent && isNotAdvertisement;\n  }).map(article => ({\n    ...article,\n    title: article.title.trim(),\n    content: article.content.trim(),\n    summary: (article.summary || article.content.substring(0, 300) + '...').trim(),\n    slug: article.slug || article.title.toLowerCase().replace(/[^a-z0-9]+/g, '-').replace(/(^-|-$)/g, ''),\n    category: article.category || 'Technology',\n    tags: Array.isArray(article.tags) ? article.tags : ['news'],\n    imageUrl: article.image_url || article.imageUrl,\n    publishedAt: article.published_at || article.publishedAt || new Date().toISOString(),\n    featured: false,\n    source: article.source || config.targetUrl.replace('https://', '').replace('http://', '').split('/')[0],\n    scrapedAt: new Date().toISOString()\n  }));\n  \n  console.log(`Quality filtering: ${articles.length} â†’ ${validArticles.length} valid articles`);\n  \n  if (validArticles.length === 0) {\n    console.warn('No articles passed quality filtering. Check site structure or selectors.');\n    return [{ json: { \n      error: 'No quality articles found', \n      targetUrl: config.targetUrl,\n      rawCount: articles.length,\n      siteAnalysis: config.siteAnalysis \n    }}];\n  }\n  \n  return validArticles.map(article => ({ json: article }));\n  \n} catch (error) {\n  console.error('Enhanced data processing error:', error);\n  return [{ json: { \n    error: 'Data processing failed', \n    details: error.message,\n    targetUrl: inputData.json.targetUrl\n  }}];\n}"
      },
      "id": "enhanced-data-processor",
      "name": "Enhanced Data Processor",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1240,
        200
      ]
    },
    {
      "parameters": {
        "batchSize": 1,
        "options": {}
      },
      "id": "quality-batch-processor",
      "name": "Quality Batch Processor",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 1,
      "position": [
        1440,
        200
      ]
    },
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "hours": 6
            }
          ]
        }
      },
      "id": "enhanced-schedule-trigger",
      "name": "Enhanced Schedule Trigger",
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1,
      "position": [
        120,
        20
      ]
    },
    {
      "parameters": {},
      "id": "batch-completer",
      "name": "Batch Completer",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [
        1660,
        400
      ]
    },
    {
      "parameters": {
        "authentication": "basicAuth",
        "requestMethod": "POST",
        "url": "http://localhost:5000/api/news",
        "jsonParameters": true,
        "options": {
          "timeout": 30000
        },
        "bodyParametersJson": "={\n  \"title\": \"{{ $json.title }}\",\n  \"content\": \"{{ $json.content }}\",\n  \"slug\": \"{{ $json.slug }}\",\n  \"summary\": \"{{ $json.summary }}\",\n  \"category\": \"{{ $json.category }}\",\n  \"tags\": {{ JSON.stringify($json.tags) }},\n  \"imageUrl\": \"{{ $json.imageUrl }}\",\n  \"publishedAt\": \"{{ $json.publishedAt }}\",\n  \"featured\": {{ $json.featured }},\n  \"source\": \"{{ $json.source }}\"\n}"
      },
      "id": "enhanced-api-integration",
      "name": "Enhanced API Integration",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        1640,
        200
      ],
      "credentials": {
        "httpBasicAuth": {
          "id": "kingsnews-enhanced-api",
          "name": "KingsNews Enhanced API"
        }
      }
    },
    {
      "parameters": {
        "values": {
          "string": [
            {
              "name": "status",
              "value": "success"
            },
            {
              "name": "operation",
              "value": "Enhanced Website Scraping"
            },
            {
              "name": "timestamp",
              "value": "={{ new Date().toISOString() }}"
            },
            {
              "name": "articlesProcessed",
              "value": "={{ $node[\"Enhanced Data Processor\"].json.length }}"
            }
          ]
        },
        "options": {}
      },
      "id": "enhanced-logger",
      "name": "Enhanced Operation Logger",
      "type": "n8n-nodes-base.set",
      "typeVersion": 1,
      "position": [
        1840,
        200
      ]
    }
  ],
  "pinData": {},
  "connections": {
    "Advanced Scraper Configuration": {
      "main": [
        [
          {
            "node": "Process Target URLs",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process Target URLs": {
      "main": [
        [
          {
            "node": "AI Structure Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "AI Structure Analysis": {
      "main": [
        [
          {
            "node": "Enhanced Scraper Selector",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enhanced Scraper Selector": {
      "main": [
        [
          {
            "node": "Enhanced Python Scraper",
            "type": "main",
            "index": 0
          },
          {
            "node": "Enhanced JavaScript Scraper",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enhanced Python Scraper": {
      "main": [
        [
          {
            "node": "Enhanced Data Processor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enhanced JavaScript Scraper": {
      "main": [
        [
          {
            "node": "Enhanced Data Processor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enhanced Data Processor": {
      "main": [
        [
          {
            "node": "Quality Batch Processor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Quality Batch Processor": {
      "main": [
        [
          {
            "node": "Enhanced API Integration",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enhanced Schedule Trigger": {
      "main": [
        [
          {
            "node": "Advanced Scraper Configuration",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Batch Completer": {
      "main": [
        [
          {
            "node": "Quality Batch Processor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enhanced API Integration": {
      "main": [
        [
          {
            "node": "Batch Completer",
            "type": "main",
            "index": 0
          },
          {
            "node": "Enhanced Operation Logger",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "techpulse-ultimate-v3.0",
  "meta": {
    "templateCredsSetupCompleted": false
  },
  "id": "techpulse-ultimate-scraper-v3",
  "tags": ["scraper", "news", "automation", "techpulse", "enhanced"]
}