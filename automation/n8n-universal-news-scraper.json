{
  "name": "Universal News Scraper - Full Content Extractor",
  "nodes": [
    {
      "parameters": {
        "values": {
          "boolean": [
            {
              "name": "enabled",
              "value": true
            }
          ],
          "string": [
            {
              "name": "targetUrl",
              "value": "https://techcrunch.com"
            },
            {
              "name": "maxArticles",
              "value": "10"
            },
            {
              "name": "scrapeDelay",
              "value": "2000"
            }
          ]
        },
        "options": {}
      },
      "id": "workflow-input-1",
      "name": "Scraper Configuration",
      "type": "n8n-nodes-base.set",
      "typeVersion": 1,
      "position": [
        240,
        300
      ]
    },
    {
      "parameters": {
        "functionCode": "const config = $node[\"Scraper Configuration\"].json;\nconst targetUrl = config.targetUrl;\nconst maxArticles = parseInt(config.maxArticles || '10');\nconst scrapeDelay = parseInt(config.scrapeDelay || '2000');\n\n// Smart site detection\nconst sitePatterns = {\n  'techcrunch.com': {\n    selectors: {\n      articles: 'article.post-block, .post-block',\n      title: 'h2.post-block__title a, .post-block__title a',\n      link: 'h2.post-block__title a, .post-block__title a',\n      summary: '.post-block__content',\n      date: '.river-byline__time',\n      category: '.post-block__category'\n    },\n    category: 'Technology'\n  },\n  'arstechnica.com': {\n    selectors: {\n      articles: 'article.tease, .tease',\n      title: 'h2 a, .tease h2 a',\n      link: 'h2 a, .tease h2 a',\n      summary: '.tease p.excerpt',\n      date: 'time',\n      category: '.overlay'\n    },\n    category: 'Technology'\n  },\n  'theverge.com': {\n    selectors: {\n      articles: 'article[data-analytics-link=\"article\"], .c-entry-box',\n      title: 'h2 a, .c-entry-box__title a',\n      link: 'h2 a, .c-entry-box__title a',\n      summary: '.c-entry-box__excerpt p',\n      date: 'time',\n      category: '.c-entry-box__category'\n    },\n    category: 'Technology'\n  },\n  'wired.com': {\n    selectors: {\n      articles: 'li.card-component, .card-component',\n      title: 'h2 a, .card-component__title a',\n      link: 'h2 a, .card-component__title a',\n      summary: '.card-component__description',\n      date: '.card-component__byline time',\n      category: '.card-component__category'\n    },\n    category: 'Technology'\n  }\n};\n\n// Determine site configuration\nconst urlParts = targetUrl.replace('https://', '').replace('http://', '').split('/')[0];\nconst domain = urlParts.replace('www.', '');\nconst siteConfig = sitePatterns[domain] || {\n  selectors: {\n    articles: 'article, .article, .post, .entry',\n    title: 'h1, h2 a, .title a',\n    link: 'h1 a, h2 a, .title a',\n    summary: 'p, .excerpt, .summary',\n    date: 'time, .date, .published',\n    category: '.category, .tag'\n  },\n  category: 'General'\n};\n\nreturn {\n  json: {\n    targetUrl,\n    maxArticles,\n    scrapeDelay,\n    siteConfig,\n    domain\n  }\n};"
      },
      "id": "config-processor-1",
      "name": "Process Configuration",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        440,
        300
      ]
    },
    {
      "parameters": {
        "functionCode": "const config = $node[\"Process Configuration\"].json;\n\nconst pythonCode = `\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nimport json\nfrom urllib.parse import urljoin, urlparse\nimport re\n\nTARGET_URL = \"${config.targetUrl}\"\nMAX_ARTICLES = ${config.maxArticles}\nSCRAPE_DELAY = ${config.scrapeDelay}\n\nSITE_CONFIG = ${JSON.stringify(config.siteConfig)}\n\nclass UniversalNewsScraper:\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip, deflate',\n            'DNT': '1',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1',\n        })\n    \n    def scrape_article_list(self):\n        try:\n            response = self.session.get(TARGET_URL, timeout=30)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            articles = []\n            article_elements = soup.select(SITE_CONFIG['selectors']['articles'])[:MAX_ARTICLES]\n            \n            for element in article_elements:\n                try:\n                    title_elem = element.select_one(SITE_CONFIG['selectors']['title'])\n                    link_elem = element.select_one(SITE_CONFIG['selectors']['link'])\n                    \n                    if not title_elem or not link_elem:\n                        continue\n                        \n                    title = title_elem.get_text().strip()\n                    link = link_elem.get('href')\n                    \n                    if not link:\n                        continue\n                        \n                    # Make absolute URL\n                    link = urljoin(TARGET_URL, link)\n                    \n                    # Extract additional info\n                    summary_elem = element.select_one(SITE_CONFIG['selectors']['summary'])\n                    summary = summary_elem.get_text().strip() if summary_elem else \"\"\n                    \n                    date_elem = element.select_one(SITE_CONFIG['selectors']['date'])\n                    published_at = None\n                    if date_elem:\n                        datetime_attr = date_elem.get('datetime')\n                        if datetime_attr:\n                            published_at = datetime_attr\n                        else:\n                            published_at = date_elem.get_text().strip()\n                    \n                    category_elem = element.select_one(SITE_CONFIG['selectors']['category'])\n                    category = SITE_CONFIG.get('category', 'General')\n                    if category_elem:\n                        category = category_elem.get_text().strip()\n                    \n                    # Extract image\n                    img_elem = element.select_one('img')\n                    image_url = None\n                    if img_elem:\n                        image_url = img_elem.get('src') or img_elem.get('data-src')\n                        if image_url:\n                            image_url = urljoin(TARGET_URL, image_url)\n                    \n                    articles.append({\n                        'title': title,\n                        'url': link,\n                        'summary': summary,\n                        'published_at': published_at,\n                        'category': category,\n                        'image_url': image_url,\n                        'source': \"${config.domain}\"\n                    })\n                    \n                except Exception as e:\n                    print(f\"Error processing article: {e}\")\n                    continue\n            \n            return articles\n            \n        except Exception as e:\n            print(f\"Error scraping article list: {e}\")\n            return []\n    \n    def scrape_full_article(self, article_url):\n        try:\n            time.sleep(SCRAPE_DELAY / 1000)  # Convert to seconds\n            response = self.session.get(article_url, timeout=30)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Remove unwanted elements\n            for unwanted in soup.select('script, style, nav, header, footer, aside, .ad, .advertisement, .sidebar'):\n                unwanted.decompose()\n            \n            # Try multiple content selectors\n            content_selectors = [\n                'article .content, article .entry-content, article .post-content',\n                '.article-content, .entry-content, .post-content',\n                'article p, .article-body p',\n                'main article, main .content',\n                '.content p'\n            ]\n            \n            full_content = \"\"\n            for selector in content_selectors:\n                content_elements = soup.select(selector)\n                if content_elements:\n                    # Get all paragraphs\n                    paragraphs = []\n                    for elem in content_elements:\n                        if elem.name == 'p':\n                            text = elem.get_text().strip()\n                            if len(text) > 50:  # Filter short paragraphs\n                                paragraphs.append(text)\n                        elif elem.find_all('p'):\n                            for p in elem.find_all('p'):\n                                text = p.get_text().strip()\n                                if len(text) > 50:\n                                    paragraphs.append(text)\n                    \n                    if paragraphs:\n                        full_content = ' '.join(paragraphs)\n                        break\n            \n            # Fallback: get all paragraphs from article\n            if not full_content:\n                article_elem = soup.select_one('article')\n                if article_elem:\n                    paragraphs = article_elem.find_all('p')\n                    full_content = ' '.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 50])\n            \n            # Clean content\n            full_content = re.sub(r'\\s+', ' ', full_content).strip()\n            \n            return full_content[:10000] if full_content else \"Content not available\"\n            \n        except Exception as e:\n            print(f\"Error scraping full article {article_url}: {e}\")\n            return \"Content not available\"\n\n# Main execution\nscraper = UniversalNewsScraper()\narticles = scraper.scrape_article_list()\n\nprint(f\"Found {len(articles)} articles\")\n\n# Scrape full content for each article\nfor article in articles:\n    print(f\"Scraping full content: {article['title'][:50]}...\")\n    article['content'] = scraper.scrape_full_article(article['url'])\n    article['slug'] = re.sub(r'[^a-zA-Z0-9]+', '-', article['title'].lower()).strip('-')\n    article['tags'] = ['news', article['category'].lower().replace(' ', '-'), article['source']]\n\nprint(json.dumps(articles))`;\n\nreturn {\n  json: {\n    pythonCode,\n    config\n  }\n};"
      },
      "id": "python-scraper-1",
      "name": "Generate Python Scraper",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        640,
        300
      ]
    },
    {
      "parameters": {
        "functionCode": "=Python Code",
        "mode": "runOnceForAllItems"
      },
      "id": "python-executor-1",
      "name": "Execute Python Scraper",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        840,
        300
      ]
    },
    {
      "parameters": {
        "functionCode": "const scrapedData = $node[\"Execute Python Scraper\"].json;\n\nlet articles = [];\n\ntry {\n  // If scrapedData is already an array, use it\n  if (Array.isArray(scrapedData)) {\n    articles = scrapedData;\n  } \n  // If it's a string, try to parse it as JSON\n  else if (typeof scrapedData === 'string') {\n    articles = JSON.parse(scrapedData);\n  }\n  // If it's an object with a data property\n  else if (scrapedData && scrapedData.data) {\n    articles = Array.isArray(scrapedData.data) ? scrapedData.data : [scrapedData.data];\n  }\n  // If it's an object with articles property\n  else if (scrapedData && scrapedData.articles) {\n    articles = scrapedData.articles;\n  }\n  else {\n    console.error('Unexpected data format:', scrapedData);\n    articles = [];\n  }\n} catch (error) {\n  console.error('Error parsing scraped data:', error);\n  articles = [];\n}\n\n// Filter and validate articles\nconst validArticles = articles.filter(article => \n  article && \n  article.title && \n  article.title.trim().length > 10 &&\n  article.content && \n  article.content.trim().length > 100\n);\n\nconsole.log(`Filtered to ${validArticles.length} valid articles`);\n\n// Transform to API format\nconst apiArticles = validArticles.map(article => ({\n  title: article.title,\n  content: article.content,\n  summary: article.summary || article.content.substring(0, 250) + '...',\n  slug: article.slug || article.title.toLowerCase().replace(/[^a-z0-9]+/g, '-').replace(/(^-|-$)/g, ''),\n  category: article.category || 'Technology',\n  tags: article.tags || ['news'],\n  imageUrl: article.image_url,\n  publishedAt: article.published_at || new Date().toISOString(),\n  featured: false,\n  source: article.source || 'Web Scraper'\n}));\n\nreturn apiArticles.map(article => ({ json: article }));"
      },
      "id": "data-processor-1",
      "name": "Process & Validate Data",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1040,
        300
      ]
    },
    {
      "parameters": {
        "batchSize": 1,
        "options": {}
      },
      "id": "batch-processor-1",
      "name": "Batch Articles",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 1,
      "position": [
        1240,
        300
      ]
    },
    {
      "parameters": {},
      "id": "batch-next-1",
      "name": "Next Batch",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [
        1460,
        500
      ]
    },
    {
      "parameters": {
        "authentication": "basicAuth",
        "requestMethod": "POST",
        "url": "http://localhost:5000/api/news",
        "jsonParameters": true,
        "options": {
          "timeout": 30000
        },
        "bodyParametersJson": "={\n  \"title\": \"{{ $json.title }}\",\n  \"content\": \"{{ $json.content }}\",\n  \"slug\": \"{{ $json.slug }}\",\n  \"summary\": \"{{ $json.summary }}\",\n  \"category\": \"{{ $json.category }}\",\n  \"tags\": {{ JSON.stringify($json.tags) }},\n  \"imageUrl\": \"{{ $json.imageUrl }}\",\n  \"publishedAt\": \"{{ $json.publishedAt }}\",\n  \"featured\": {{ $json.featured }},\n  \"source\": \"{{ $json.source }}\"\n}"
      },
      "id": "api-poster-1",
      "name": "POST to KingsNews API",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        1440,
        300
      ],
      "credentials": {
        "httpBasicAuth": {
          "id": "scraper-auth",
          "name": "KingsNews API Auth"
        }
      }
    }
  ],
  "pinData": {},
  "connections": {
    "Scraper Configuration": {
      "main": [
        [
          {
            "node": "Process Configuration",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process Configuration": {
      "main": [
        [
          {
            "node": "Generate Python Scraper",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Python Scraper": {
      "main": [
        [
          {
            "node": "Execute Python Scraper",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Execute Python Scraper": {
      "main": [
        [
          {
            "node": "Process & Validate Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process & Validate Data": {
      "main": [
        [
          {
            "node": "Batch Articles",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Batch Articles": {
      "main": [
        [
          {
            "node": "POST to KingsNews API",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Next Batch": {
      "main": [
        [
          {
            "node": "Batch Articles",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "POST to KingsNews API": {
      "main": [
        [
          {
            "node": "Next Batch",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "universal-scraper-v1",
  "meta": {
    "templateCredsSetupCompleted": false
  },
  "id": "universal-news-scraper",
  "tags": ["scraper", "news", "automation", "python"]
}